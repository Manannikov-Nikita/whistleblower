# MASTER PLAN — Primary
# Проект: Транскрибация встречи → диаризация → саммари

## 0) Контекст и допущения
- Платформа: macOS (Apple Silicon).
- Захват: запись через Chrome расширение (папка `chrome_audio`), результат — `.webm` файл.
- Транскрибация: локальный Whisper, для русского по качеству подходит `medium`.
- Диаризация: pyannote.audio, допустим упрощенный пайплайн (требуется HF токен).
- Язык речи: русский, длительность 30–60 минут.
- Вывод: Markdown, хронологический порядок без точных тайм-кодов.
- Саммари: OpenAI API, bullet points по каждому актору + финальные договоренности + “обсудили, но не решили”.
- Логи допустимы, но не хранить тяжелые данные без необходимости.
- Интерфейс: CLI.

## 1) Цели
- Получать аудио через Chrome расширение и запускать обработку в CLI.
- Выполнить диаризацию для разделения по говорящим.
- Сгенерировать структурированное саммари через OpenAI API.
- Сохранить результат в Markdown с минимумом “тяжелых” артефактов на диске.

## 2) Функциональные требования
### 2.1 Захват аудио
- Запись звука вкладки (и опционально микрофона) через Chrome расширение.
- CLI принимает готовый файл записи (`.webm`) и обрабатывает его оффлайн.

### 2.2 Транскрибация (Whisper локально)
- Использовать легкую модель (например, `tiny` или `base`) для MVP.
- Пакетная обработка чанков (например 10–30 сек) с объединением в общий текст.
- Учитывать порядок фрагментов, хранить метаданные для выравнивания (start/end).
- Сохранять сегменты распознавания в `transcript_segments.json`.

### 2.3 Диаризация (pyannote.audio)
- Оффлайн-диаризация по итоговому аудиофайлу (или агрегированным чанкам).
- Результат: сегменты с метками speaker-1, speaker-2 и т.д.
- Примерная синхронизация сегментов диаризации с текстом по временным меткам.
- Для модели нужен Hugging Face токен (HUGGINGFACE_TOKEN).
- Выход: `diarization/segments.json` + RTTM.

### 2.4 Слияние диаризации и транскрипта
- Поиск спикера для каждого сегмента Whisper по пересечению интервалов.
- Склейка соседних сегментов одного спикера (настраиваемый `max_gap`).
- Выход: `speaker_transcript.md` (спикер → реплика).

### 2.5 Саммари (OpenAI API)
- Вход: полный текст с разметкой спикеров.
- Выход: Markdown с разделами:
  - По каждому актеру: bullet points.
  - Финальные договоренности/решения.
  - Обсуждали, но не приняли решение/отложили.

### 2.6 Экспорт и хранение
- Markdown-отчет сохраняется в `/output`.
- Хранить промежуточные файлы минимально: 
  - RAW аудио удаляется после успешного отчета (настраиваемо).
  - Логи короткие, без дублирования крупных данных.

## 3) Нефункциональные требования
- Простая установка и запуск CLI.
- Предсказуемая нагрузка на CPU/GPU для Apple Silicon.
- Низкий риск “переполнения диска”.
- Стабильность при часовых сессиях.

## 4) Архитектура (высокоуровневая)

**Поток данных**
1) Аудио из браузера → Chrome расширение → `.webm` файл.
2) CLI обрабатывает файл записи.
3) Whisper локально → текст + временные интервалы.
4) Диаризация через pyannote по аудио → сегменты спикеров.
5) Слияние: текст + спикеры → `speaker_transcript.md`.
6) OpenAI API → саммари в Markdown.

**Компоненты**
- `audio_ingest`: обработка файла записи (WebM/Opus) для ASR/диаризации.
- `asr_whisper`: транскрибация чанков, склейка.
- `diarization_pyannote`: оффлайн-диаризация.
- `merge_transcript`: сопоставление сегментов диаризации и текста, вывод `speaker_transcript.md`.
- `summarizer_openai`: генерация итогового отчета.
- `cli`: управление запуском/остановкой, конфиг.

## 5) Поток выполнения (MVP)
1) Записать встречу через Chrome расширение (получить `.webm` файл).
2) `cli transcribe` → `diarize` → `merge-transcript` → `summarize` (по файлу записи).

## 6) Конфигурация
- `.env`:
  - `OPENAI_API_KEY`
  - `OPENAI_MODEL` (по умолчанию `gpt-4o-mini`)
  - `HUGGINGFACE_TOKEN`
  - `WHISPER_MODEL` (по умолчанию `medium`, можно понизить ради скорости)
  - `OUTPUT_DIR` (по умолчанию `./output`)
  - `KEEP_RAW_AUDIO` (true/false)
  - `CHUNK_SEC` (10–30)

## 7) Риски и оговорки
- Whisper `tiny` может ошибаться в русской речи; потребуется баланс скорости/качества.
- Диаризация pyannote чувствительна к качеству микса; точность “спикер-к-спикеру” будет ориентировочной.
- PyTorch 2.6+ требует safe_globals для загрузки pyannote чекпоинтов (учтено в коде).
- Без точных тайм-кодов невозможна идеальная привязка спикера к каждой фразе.

## 8) Минимальные зависимости (черновик)
- Python 3.9–3.12
- Whisper (локально) или faster-whisper для скорости
- pyannote.audio
- Для диаризации: `uv sync`

## 9) Вопросы для уточнения (если появятся)
- Нужно ли сохранять промежуточный transcript в файл до финального summary?
- Требуется ли поддержка нескольких встреч подряд (batch)?
